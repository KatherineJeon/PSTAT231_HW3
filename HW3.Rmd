---
title: "HW3"
output: html_document
date: '2022-04-17'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Importing data and loading necessary packages.
```{r}
titanic <- read.csv(file = 'data/titanic.csv')
library(ggplot2)
library(tidymodels)
library(tidyverse)
library(corrr)
library(discrim)
set.seed(45)
```

Changing some variables into factors.
```{r}
titanic$survived <- factor(titanic$survived, levels = c('Yes', 'No'))
titanic$pclass <- factor(titanic$pclass)
levels(titanic$survived)
```


## Question 1

```{r}
titanic_split <- initial_split(titanic, prop = 0.8,
                                strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
```

```{r}
head(titanic_train)
```

```{r}
sum(is.na(titanic_train))
mean(is.na(titanic_train))
```


Since we are trying to train the model to predict 'survived' factor, We need to see each data with the same ratio with the original data.

## Question 2

```{r}
ggplot(titanic_train, aes(x = survived)) + geom_bar()
```

According to the graph, 'Yes' takes up about 40% of the data and 'No' takes up about 60%.


## Question 3
```{r}
cor_titanic <- titanic_train %>%
  select(is.numeric) %>%
  correlate()
cor_titanic
```

```{r}
rplot(cor_titanic)
```

According to visualized plot, one of the distinctive relationships is the correlation between family-related values and age. This can be explained by the general life-cycle because people are less likely to have siblings, spouse, parents, and children aboard with them as they become older. I think mid-aged or elderly people are more likely to travel with their family members. Considering many circumstances that can happen in person's life, the number of direct family members that can go on a cruise trip with would decrease. And in same sense, 'sib_sp' and 'parch' variables have positive correlation because both indicate the number of direct family member aboard. 


## Question 4

```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare , data = titanic_train) %>%
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ starts_with("sex"):fare + age:fare)
```


## Question 5

```{r}
glm_model <- logistic_reg() %>% 
  set_engine("glm") %>%
  set_mode("classification")

glm_model
```

```{r}
glm_wflow <- workflow() %>% 
  add_model(glm_model) %>% 
  add_recipe(titanic_recipe)

glm_fit <- fit(glm_wflow, titanic_train)
glm_fit %>%
  tidy()
```


## Question 6

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

lda_fit <- fit(lda_wkflow, titanic_train)
```


## Question 7

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)

qda_fit <- fit(qda_wkflow, titanic_train)
```


## Question 8

```{r}
library(klaR)
nb_mod <- naive_Bayes() %>% 
  set_mode("classification") %>% 
  set_engine("klaR") %>% 
  set_args(usekernel = FALSE) 

nb_wkflow <- workflow() %>% 
  add_model(nb_mod) %>% 
  add_recipe(titanic_recipe)

nb_fit <- fit(nb_wkflow, titanic_train)
```


## Question 9

```{r}
glm_train <- predict(glm_fit, new_data = titanic_train)
glm_train <- bind_cols(glm_train, titanic_train)

lda_train <- predict(lda_fit, new_data = titanic_train)
lda_train <- bind_cols(lda_train, titanic_train)

qda_train <- predict(qda_fit, new_data = titanic_train)
qda_train <- bind_cols(qda_train, titanic_train)

nb_train <- predict(nb_fit, new_data = titanic_train)
nb_train <- bind_cols(nb_train, titanic_train)

head(glm_train)
head(lda_train)
head(qda_train)
head(nb_train)
```


```{r}
glm_acc <- augment(glm_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)

lda_acc <- augment(lda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)

qda_acc <- augment(qda_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)

nb_acc <- augment(nb_fit, new_data = titanic_train) %>%
  accuracy(truth = survived, estimate = .pred_class)

accuracies <- c(log_reg_acc$.estimate, lda_acc$.estimate, 
                nb_acc$.estimate, qda_acc$.estimate)
models <- c("Logistic Regression", "LDA", "Naive Bayes", "QDA")
results <- tibble(accuracies = accuracies, models = models)
results %>% 
  arrange(-accuracies)
```
Logistic regression has the highest accuracy.


## Question 10

```{r}
cm <- augment(glm_fit, new_data = titanic_test) %>%
  conf_mat(truth = survived, estimate = .pred_class) 
```

```{r}
autoplot(cm, type = "heatmap")
```


```{r}
predict(glm_fit, new_data = titanic_test, type = "prob")

multi_metric <- metric_set(accuracy, sensitivity, specificity)

augment(glm_fit, new_data = titanic_test) %>%
  roc_curve(survived, .pred_Yes) %>%
  autoplot()
```


```{r}
augment(glm_fit, new_data = titanic_test) %>%
  roc_auc(truth = survived, estimate = .pred_Yes)
```


## Question 11
$$
\begin{aligned}
p &= \frac{e^z} {1 + e^z} \\
&= 1 - \frac{1} {1 + e^z} \\
1 - p &= \frac{1} {1 + e^z} \\
1 + e^z &= \frac{1} {1 - p}  = \frac{1 -p} {1-p} + \frac{p} {1 - p}\\
e^z &= \frac{p} {1-p} \\
z &= log\frac{p} {1 - p} \\
z(p) &= ln(\frac{p} {1 - p})
\end{aligned}
$$


## Question 12

$$
\frac{odds(x_1 +2)} {odds(x_1)} = \frac{e ^{\beta_0 + \beta_1 (x_1 +2)}} {e ^{\beta_0 + \beta_1 x_1}}
$$


